{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from generator_class_v2 import Dataset,my_collate,AverageMeter\n",
    "\n",
    "from torch_baidu_ctc import CTCLoss\n",
    "#from torch.nn import CTCLoss\n",
    "\n",
    "from ocr_model_v2 import cnn_attention_ocr\n",
    "#from torch.nn import CTCLoss\n",
    "import torch\n",
    "import torch.nn  as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau,ExponentialLR\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.figure()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "from evaluation import wer_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3543584"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Evaluation methods \n",
    "cnn=cnn_attention_ocr(model_dim=64,nclasses=93,n_layers=8)\n",
    "cnn=cnn.cuda().train()\n",
    "count_parameters(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=\"logs2/new_generator_transfer_to_heavy_aug_h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctc_loss = CTCLoss(reduction=\"mean\",average_frames=True)\n",
    "#Good initial is 5e5 \n",
    "optimizer = optim.Adam(cnn.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ave_total_loss = AverageMeter()\n",
    "CER_total= AverageMeter()\n",
    "\n",
    "n_iter=0\n",
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn.load_state_dict(torch.load(\"contiued_training_on_heavy_agment_CER4%_22_6kite.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5c11f092e0f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m#Targets have to be CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m#Get the Lengths/2 becase this is how much we dwonsample the width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epochs in range(10000):\n",
    "    #Initing the dataset actually downloads a bunch of data and creats the images \n",
    "    print(\"getting data\")\n",
    "    ds=Dataset(batch_size,epoch_size=1000,random_strings=True,num_words=5)\n",
    "    #Then we set up our own custom dataloader, with a custom collate, which packs the data\n",
    "    #(does the padding) Should work with variable number of widths. \n",
    "\n",
    "    trainset = DataLoader(dataset=ds,\n",
    "                          #Multi processing worker leads to an error with CTC Loss\n",
    "                          #num_workers=6,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=my_collate)\n",
    "    gen = iter(trainset)\n",
    "    print(\"train start\")\n",
    "    for ge in gen:\n",
    "        if ge[0].shape[3]<1000:\n",
    "            #break\n",
    "            #DONT FORGET THE ZERO GRAD!!!!\n",
    "            optimizer.zero_grad()\n",
    "            #Get Predictions\n",
    "            log_probs = cnn(ge[0]).permute((2,0,1))\n",
    "\n",
    "            #Targets have to be CPU \n",
    "            targets = ge[1].cpu()\n",
    "\n",
    "            #Get the Lengths/2 becase this is how much we downsample the width\n",
    "            input_lengths = ge[2]/2\n",
    "            target_lengths = ge[3]\n",
    "            #Get the CTC Loss \n",
    "            loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Save Loss \n",
    "            ave_total_loss.update(loss.data.item())\n",
    "            #print(ave_total_loss.average())\n",
    "            #Write it to Tensorborad \n",
    "            writer.add_scalar(\"total_loss\", ave_total_loss.average(), n_iter) \n",
    "            n_iter=n_iter+1\n",
    "            \n",
    "            \n",
    "            #Here we Calculate the Character error rate\n",
    "            cum_len=np.cumsum(target_lengths)\n",
    "            targets=np.split(ge[1].cpu(),cum_len[:-1])\n",
    "            wer_list=[]\n",
    "            for j in range(log_probs.shape[1]):\n",
    "                wer_list.append(wer_eval(log_probs[:,j,:][0:input_lengths[j],:],targets[j]))\n",
    "            #print(np.average(wer_list))\n",
    "            \n",
    "            CER_total.update(np.average(wer_list))\n",
    "            writer.add_scalar(\"CER\", CER_total.average(), n_iter)\n",
    "            #print(CER_total.average())\n",
    "            del log_probs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m=cnn.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(m,\"contiued_training_on_heavy_agment_CER2%_40kite.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorchenv)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
