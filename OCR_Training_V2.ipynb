{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from generator_class_v2 import Dataset,my_collate,AverageMeter\n",
    "\n",
    "from torch_baidu_ctc import CTCLoss\n",
    "#from torch.nn import CTCLoss\n",
    "\n",
    "from ocr_model_v2 import cnn_attention_ocr\n",
    "#from torch.nn import CTCLoss\n",
    "import torch\n",
    "import torch.nn  as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau,ExponentialLR\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.figure()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "from evaluation import wer_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3543584"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Evaluation methods \n",
    "cnn=cnn_attention_ocr(model_dim=64,nclasses=93,n_layers=8)\n",
    "cnn=cnn.cuda().train()\n",
    "count_parameters(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=\"/home/leander/AI/repos/OCR-CNN/logs2/retry_no_aug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctc_loss = CTCLoss(reduction=\"mean\",average_frames=True)\n",
    "#Good initial is 5e5 \n",
    "optimizer = optim.Adam(cnn.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ave_total_loss = AverageMeter()\n",
    "CER_total= AverageMeter()\n",
    "\n",
    "n_iter=0\n",
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn.load_state_dict(torch.load(\"contiued_training_on_heavy_agment_CER4%_22_6kite.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data\n",
      "train start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leander/AI/repos/Pytorch-OCR-Fully-Conv/evaluation.py:41: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return we/len(preds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data\n",
      "train start\n",
      "getting data\n",
      "train start\n"
     ]
    }
   ],
   "source": [
    "for epochs in range(10000):\n",
    "    #Initing the dataset actually downloads a bunch of data and creats the images \n",
    "    print(\"getting data\")\n",
    "    ds=Dataset(batch_size,epoch_size=1000,random_strings=True,num_words=5)\n",
    "    #Then we set up our own custom dataloader, with a custom collate, which packs the data\n",
    "    #(does the padding) Should work with variable number of widths. \n",
    "\n",
    "    trainset = DataLoader(dataset=ds,\n",
    "                          #Multi processing worker leads to an error with CTC Loss\n",
    "                          #num_workers=6,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=my_collate)\n",
    "    gen = iter(trainset)\n",
    "    print(\"train start\")\n",
    "    for ge in gen:\n",
    "        if ge[0].shape[3]<800:\n",
    "            #break\n",
    "            #DONT FORGET THE ZERO GRAD!!!!\n",
    "            optimizer.zero_grad()\n",
    "            #Get Predictions\n",
    "            log_probs = cnn(ge[0]).permute((2,0,1))\n",
    "\n",
    "            #Targets have to be CPU \n",
    "            targets = ge[1].cpu()\n",
    "\n",
    "            #Get the Lengths/2 becase this is how much we downsample the width\n",
    "            input_lengths = ge[2]/2\n",
    "            target_lengths = ge[3]\n",
    "            #Get the CTC Loss \n",
    "            loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Save Loss \n",
    "            ave_total_loss.update(loss.data.item())\n",
    "            #print(ave_total_loss.average())\n",
    "            #Write it to Tensorborad \n",
    "            writer.add_scalar(\"total_loss\", ave_total_loss.average(), n_iter) \n",
    "            n_iter=n_iter+1\n",
    "            \n",
    "            \n",
    "            #Here we Calculate the Character error rate\n",
    "            cum_len=np.cumsum(target_lengths)\n",
    "            targets=np.split(ge[1].cpu(),cum_len[:-1])\n",
    "            wer_list=[]\n",
    "            for j in range(log_probs.shape[1]):\n",
    "                wer_list.append(wer_eval(log_probs[:,j,:][0:input_lengths[j],:],targets[j]))\n",
    "            #print(np.average(wer_list))\n",
    "            \n",
    "            CER_total.update(np.average(wer_list))\n",
    "            writer.add_scalar(\"CER\", CER_total.average(), n_iter)\n",
    "            #print(CER_total.average())\n",
    "            del log_probs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m=cnn.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(m,\"contiued_training_on_heavy_agment_CER2%_40kite.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorchenv)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
