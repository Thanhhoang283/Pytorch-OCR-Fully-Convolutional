{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#General Imports\n",
    "import torch\n",
    "import torch.nn  as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "#Load fake, non handwritten generator \n",
    "from fake_texts.pytorch_dataset_fake import Dataset\n",
    "\n",
    "#Import the loss from baidu \n",
    "from torch_baidu_ctc import CTCLoss\n",
    "\n",
    "#Import the model \n",
    "from fully_conv_model import cnn_attention_ocr\n",
    "\n",
    "#Helper to count params\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "#Evaluation function preds_to_integer\n",
    "from evaluation import wer_eval,preds_to_integer,show,my_collate,AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up Tensorboard writer for current test\n",
    "writer = SummaryWriter(log_dir=\"/home/leander/AI/repos/OCR-CNN/logs2/test_heavy_augment_new_128_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13952768"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Set up model. \n",
    "cnn=cnn_attention_ocr(model_dim=128,nclasses=93,n_layers=8)\n",
    "cnn=cnn.cuda().train()\n",
    "count_parameters(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cnn=cnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CTC Loss \n",
    "ctc_loss = CTCLoss(reduction=\"mean\",average_frames=True)\n",
    "#Optimizer: Good initial is 5e5 \n",
    "optimizer = optim.Adam(cnn.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We keep track of the Average loss and CER \n",
    "ave_total_loss = AverageMeter()\n",
    "CER_total= AverageMeter()\n",
    "\n",
    "n_iter=0\n",
    "batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cnn.load_state_dict(torch.load(\"8_layers_continued_on_blanks_340k.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data\n",
      "train start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leander/AI/repos/Pytorch-OCR-Fully-Conv/evaluation.py:42: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return we/len(preds)\n"
     ]
    }
   ],
   "source": [
    "for epochs in range(10000):\n",
    "    #Initing the dataset actually downloads a bunch of data and creats the images The generator actually has a bunch more \n",
    "    #Options\n",
    "    print(\"getting data\")\n",
    "    if random.random()<0.06:\n",
    "\n",
    "    #PROBLEMO I CNAT JUST HAVE A LOW WIDTH OTEHRWEISE SHIT WILL GET CUT OUT!\n",
    "\n",
    "        width=800\n",
    "        print(width)\n",
    "        if random.random()<0.5:\n",
    "            #Means center alignment (most reasonable)\n",
    "            alignment=1\n",
    "        else:\n",
    "            if random.random()<0.5:\n",
    "                alignment=0\n",
    "            else:\n",
    "                alignment=2\n",
    "\n",
    "    else:\n",
    "        width=-1\n",
    "        alignment=1\n",
    "            \n",
    "\n",
    "    \n",
    "    #width=-1\n",
    "    #alignment=1\n",
    "    ds=Dataset(batch_size,epoch_size=1500,random_strings=True,num_words=5,transform=False,width=width,alignment=alignment)\n",
    "    #Then we set up our own custom dataloade500r, with a custom collate, which packs the data\n",
    "    #(does the padding) Should work with variable number of widths. \n",
    "    \n",
    "    #Multiple worker leads to crash with CTC loss \n",
    "    trainset = DataLoader(dataset=ds,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=my_collate)\n",
    "    gen = iter(trainset)\n",
    "    print(\"train start\")\n",
    "    for i,ge in enumerate(gen):\n",
    "        \n",
    "        #to avoid OOM \n",
    "        if ge[0].shape[3]<=800:\n",
    "\n",
    "            #DONT FORGET THE ZERO GRAD!!!!\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Get Predictions, permuted for CTC loss \n",
    "            log_probs = cnn(ge[0]).permute((2,0,1))\n",
    "\n",
    "            #Targets have to be CPU for baidu loss \n",
    "            targets = ge[1].cpu()\n",
    "\n",
    "            #Get the Lengths/2 becase this is how much we downsample the width\n",
    "            input_lengths = ge[2]/2\n",
    "            target_lengths = ge[3]\n",
    "            \n",
    "            #Get the CTC Loss \n",
    "            loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n",
    "            \n",
    "            #Then backward and step \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Save Loss in averagemeter and write to tensorboard \n",
    "            ave_total_loss.update(loss.data.item())\n",
    "            writer.add_scalar(\"total_loss\", ave_total_loss.average(), n_iter) \n",
    "            \n",
    "            \n",
    "            #Here we Calculate the Character error rate\n",
    "            cum_len=np.cumsum(target_lengths)\n",
    "            targets=np.split(ge[1].cpu(),cum_len[:-1])\n",
    "            wer_list=[]\n",
    "            for j in range(log_probs.shape[1]):\n",
    "                wer_list.append(wer_eval(log_probs[:,j,:][0:input_lengths[j],:],targets[j]))\n",
    "            \n",
    "            #Here we save an example together with its decoding and truth\n",
    "            #Only if it is positive \n",
    "            \n",
    "            if np.average(wer_list)>0.1:\n",
    "\n",
    "                max_elem=np.argmax(wer_list)\n",
    "                max_value=np.max(wer_list)\n",
    "                max_image=ge[0][max_elem].cpu()\n",
    "                max_target=targets[max_elem]\n",
    "                \n",
    "                max_target=[ds.decode_dict[x] for x in max_target.tolist()]\n",
    "                max_target=\"\".join(max_target)\n",
    "\n",
    "                ou=preds_to_integer(log_probs[:,max_elem,:])\n",
    "                max_preds=[ds.decode_dict[x] for x in ou]\n",
    "                max_preds=\"\".join(max_preds)\n",
    "                \n",
    "                writer.add_text(\"label\",max_target,n_iter)\n",
    "                writer.add_text(\"pred\",max_preds,n_iter)\n",
    "                writer.add_image(\"img\",ge[0][max_elem].detach().cpu().numpy(),n_iter)\n",
    "                \n",
    "                #gen.close()\n",
    "                #break\n",
    "                \n",
    "            #Might become infinite \n",
    "            if np.average(wer_list)< 10: \n",
    "                CER_total.update(np.average(wer_list))\n",
    "                writer.add_scalar(\"CER\", CER_total.average(), n_iter)\n",
    "                \n",
    "            n_iter=n_iter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorchenv)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
