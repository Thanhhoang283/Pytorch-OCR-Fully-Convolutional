{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#General Imports\n",
    "import torch\n",
    "import torch.nn  as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "#Load fake, non handwritten generator \n",
    "from fake_texts.pytorch_dataset_fake_2 import Dataset\n",
    "\n",
    "#Import the loss from baidu \n",
    "from torch_baidu_ctc import CTCLoss\n",
    "\n",
    "#Import the model \n",
    "from fully_conv_model import cnn_attention_ocr\n",
    "\n",
    "#Helper to count params\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "#Evaluation function preds_to_integer\n",
    "from evaluation import wer_eval,preds_to_integer,show,my_collate,AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up Tensorboard writer for current test\n",
    "writer = SummaryWriter(log_dir=\"/home/leander/AI/repos/OCR-CNN/logs2/new_generator_augments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13926066"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Set up model. \n",
    "cnn=cnn_attention_ocr(model_dim=128,nclasses=67,n_layers=8)\n",
    "cnn=cnn.cuda().train()\n",
    "count_parameters(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cnn=cnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CTC Loss \n",
    "ctc_loss = CTCLoss(reduction=\"mean\",average_frames=True)\n",
    "#Optimizer: Good initial is 5e5 \n",
    "optimizer = optim.Adam(cnn.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We keep track of the Average loss and CER \n",
    "ave_total_loss = AverageMeter()\n",
    "CER_total= AverageMeter()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iter=0\n",
    "batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#torch.save(cnn.state_dict(), \"128_100k_10cer_just_started_augment.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "cnn.load_state_dict(torch.load(\"128_100k_10cer_just_started_augment.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds=Dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = DataLoader(dataset=ds,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=False,\n",
    "                      collate_fn=my_collate)\n",
    "gen = iter(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start\n"
     ]
    }
   ],
   "source": [
    "for epochs in range(10000):\n",
    "\n",
    "    gen = iter(trainset)\n",
    "    print(\"train start\")\n",
    "    for i,ge in enumerate(gen):\n",
    "        \n",
    "        #to avoid OOM \n",
    "        if ge[0].shape[3]<=800:\n",
    "\n",
    "            #DONT FORGET THE ZERO GRAD!!!!\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Get Predictions, permuted for CTC loss \n",
    "            log_probs = cnn(ge[0]).permute((2,0,1))\n",
    "\n",
    "            #Targets have to be CPU for baidu loss \n",
    "            targets = ge[1].cpu()\n",
    "\n",
    "            #Get the Lengths/2 becase this is how much we downsample the width\n",
    "            input_lengths = ge[2]/2\n",
    "            target_lengths = ge[3]\n",
    "            \n",
    "            #Get the CTC Loss \n",
    "            loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n",
    "            \n",
    "            #Then backward and step \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Save Loss in averagemeter and write to tensorboard \n",
    "            ave_total_loss.update(loss.data.item())\n",
    "            writer.add_scalar(\"total_loss\", ave_total_loss.average(), n_iter) \n",
    "            \n",
    "            \n",
    "            #Here we Calculate the Character error rate\n",
    "            cum_len=np.cumsum(target_lengths)\n",
    "            targets=np.split(ge[1].cpu(),cum_len[:-1])\n",
    "            wer_list=[]\n",
    "            for j in range(log_probs.shape[1]):\n",
    "                wer_list.append(wer_eval(log_probs[:,j,:][0:input_lengths[j],:],targets[j]))\n",
    "            \n",
    "            #Here we save an example together with its decoding and truth\n",
    "            #Only if it is positive \n",
    "            \n",
    "            if np.average(wer_list)>0.1:\n",
    "\n",
    "                max_elem=np.argmax(wer_list)\n",
    "                max_value=np.max(wer_list)\n",
    "                max_image=ge[0][max_elem].cpu()\n",
    "                max_target=targets[max_elem]\n",
    "                \n",
    "                max_target=[ds.decode_dict[x] for x in max_target.tolist()]\n",
    "                max_target=\"\".join(max_target)\n",
    "\n",
    "                ou=preds_to_integer(log_probs[:,max_elem,:])\n",
    "                max_preds=[ds.decode_dict[x] for x in ou]\n",
    "                max_preds=\"\".join(max_preds)\n",
    "                \n",
    "                writer.add_text(\"label\",max_target,n_iter)\n",
    "                writer.add_text(\"pred\",max_preds,n_iter)\n",
    "                writer.add_image(\"img\",ge[0][max_elem].detach().cpu().numpy(),n_iter)\n",
    "                \n",
    "                #gen.close()\n",
    "                #break\n",
    "                \n",
    "            #Might become infinite \n",
    "            if np.average(wer_list)< 10: \n",
    "                CER_total.update(np.average(wer_list))\n",
    "                writer.add_scalar(\"CER\", CER_total.average(), n_iter)\n",
    "                \n",
    "            n_iter=n_iter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorchenv)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
