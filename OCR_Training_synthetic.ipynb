{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#General Imports\n",
    "import torch\n",
    "import torch.nn  as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "#Load fake, non handwritten generator \n",
    "from fake_texts.pytorch_dataset_fake import Dataset\n",
    "\n",
    "#Import the loss from baidu \n",
    "from torch_baidu_ctc import CTCLoss\n",
    "\n",
    "#Import the model \n",
    "from fully_conv_model import cnn_attention_ocr\n",
    "\n",
    "#Helper to count params\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "#Evaluation function preds_to_integer\n",
    "from evaluation import wer_eval,preds_to_integer,show,my_collate,AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up Tensorboard writer for current test\n",
    "writer = SummaryWriter(log_dir=\"/home/leander/AI/repos/OCR-CNN/logs2/test_normal_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3543584"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Set up model. \n",
    "cnn=cnn_attention_ocr(model_dim=64,nclasses=93,n_layers=8)\n",
    "cnn=cnn.cuda().train()\n",
    "count_parameters(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cnn=cnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CTC Loss \n",
    "ctc_loss = CTCLoss(reduction=\"mean\",average_frames=True)\n",
    "#Optimizer: Good initial is 5e5 \n",
    "optimizer = optim.Adam(cnn.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We keep track of the Average loss and CER \n",
    "ave_total_loss = AverageMeter()\n",
    "CER_total= AverageMeter()\n",
    "\n",
    "n_iter=0\n",
    "batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn.load_state_dict(torch.load(\"8_layers_continued_on_blanks_340k.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start\n",
      "train start\n",
      "train start\n",
      "train start\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-29f49e5636a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m#Then backward and step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m#Save Loss in averagemeter and write to tensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epochs in range(10000):\n",
    "    #Initing the dataset actually downloads a bunch of data and creats the images The generator actually has a bunch more \n",
    "    #Options\n",
    "    \n",
    "    if random.random()<0.06:\n",
    "\n",
    "    #PROBLEMO I CNAT JUST HAVE A LOW WIDTH OTEHRWEISE SHIT WILL GET CUT OUT!\n",
    "\n",
    "        width=800\n",
    "        print(width)\n",
    "        if random.random()<0.5:\n",
    "            #Means center alignment (most reasonable)\n",
    "            alignment=1\n",
    "        else:\n",
    "            if random.random()<0.5:\n",
    "                alignment=0\n",
    "            else:\n",
    "                alignment=2\n",
    "\n",
    "    else:\n",
    "        width=-1\n",
    "        alignment=1\n",
    "            \n",
    "\n",
    "    \n",
    "    #width=-1\n",
    "    #alignment=1\n",
    "    ds=Dataset(batch_size,epoch_size=500,random_strings=True,num_words=5,transform=True,width=width,alignment=alignment)\n",
    "    #Then we set up our own custom dataloade500r, with a custom collate, which packs the data\n",
    "    #(does the padding) Should work with variable number of widths. \n",
    "    \n",
    "    #Multiple worker leads to crash with CTC loss \n",
    "    trainset = DataLoader(dataset=ds,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=my_collate)\n",
    "    gen = iter(trainset)\n",
    "    print(\"train start\")\n",
    "    for i,ge in enumerate(gen):\n",
    "        \n",
    "        #to avoid OOM \n",
    "        if ge[0].shape[3]<=800:\n",
    "\n",
    "            #DONT FORGET THE ZERO GRAD!!!!\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Get Predictions, permuted for CTC loss \n",
    "            log_probs = cnn(ge[0]).permute((2,0,1))\n",
    "\n",
    "            #Targets have to be CPU for baidu loss \n",
    "            targets = ge[1].cpu()\n",
    "\n",
    "            #Get the Lengths/2 becase this is how much we downsample the width\n",
    "            input_lengths = ge[2]/2\n",
    "            target_lengths = ge[3]\n",
    "            \n",
    "            #Get the CTC Loss \n",
    "            loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n",
    "            \n",
    "            #Then backward and step \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Save Loss in averagemeter and write to tensorboard \n",
    "            ave_total_loss.update(loss.data.item())\n",
    "            writer.add_scalar(\"total_loss\", ave_total_loss.average(), n_iter) \n",
    "            \n",
    "            \n",
    "            #Here we Calculate the Character error rate\n",
    "            cum_len=np.cumsum(target_lengths)\n",
    "            targets=np.split(ge[1].cpu(),cum_len[:-1])\n",
    "            wer_list=[]\n",
    "            for j in range(log_probs.shape[1]):\n",
    "                wer_list.append(wer_eval(log_probs[:,j,:][0:input_lengths[j],:],targets[j]))\n",
    "            \n",
    "            #Here we save an example together with its decoding and truth\n",
    "            #Only if it is positive \n",
    "            \n",
    "            if np.average(wer_list)>0.1:\n",
    "\n",
    "                max_elem=np.argmax(wer_list)\n",
    "                max_value=np.max(wer_list)\n",
    "                max_image=ge[0][max_elem].cpu()\n",
    "                max_target=targets[max_elem]\n",
    "                \n",
    "                max_target=[ds.decode_dict[x] for x in max_target.tolist()]\n",
    "                max_target=\"\".join(max_target)\n",
    "\n",
    "                ou=preds_to_integer(log_probs[:,max_elem,:])\n",
    "                max_preds=[ds.decode_dict[x] for x in ou]\n",
    "                max_preds=\"\".join(max_preds)\n",
    "                \n",
    "                writer.add_text(\"label\",max_target,n_iter)\n",
    "                writer.add_text(\"pred\",max_preds,n_iter)\n",
    "                writer.add_image(\"img\",ge[0][max_elem].detach().cpu().numpy(),n_iter)\n",
    "                \n",
    "                #gen.close()\n",
    "                #break\n",
    "                \n",
    "            #Might become infinite \n",
    "            if np.average(wer_list)< 10: \n",
    "                CER_total.update(np.average(wer_list))\n",
    "                writer.add_scalar(\"CER\", CER_total.average(), n_iter)\n",
    "                \n",
    "            n_iter=n_iter+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorchenv)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
